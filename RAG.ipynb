{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMSDxKb+rwFnhhnVoOYaQDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sajidcodecrack/Multilingual-RAG-System/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install All Necessary Packages\n",
        "print(\"Installing Tesseract OCR engine, language pack, and Python libraries...\")\n",
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y tesseract-ocr tesseract-ocr-ben -qq\n",
        "!pip install -q pytesseract pypdf langchain sentence-transformers chromadb google-generativeai python-dotenv PyMuPDF\n",
        "print(\" All necessary packages are installed.\")"
      ],
      "metadata": {
        "id": "Ps7bNaCklmT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "import fitz  # PyMuPDF librarie\n",
        "from google.colab import files\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io   #Optimizing the Input and Out Operation smoothly\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf3j8-3km2a2",
        "outputId": "9496b7d7-9d7a-48d2-b40c-3febd69b3fdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring gemini api\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\" Google API Key configured successfully!\")\n",
        "except (ImportError, KeyError):\n",
        "\n",
        "    print(\" Google API Key not found in Colab secrets\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aquFxAVQm8Pb",
        "outputId": "039d7af2-769f-4f0d-8937-fd94ddc63a84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Google API Key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the PDF File\n",
        "print(\"\\n---- Upload the file -----\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    print(f\"Successfully uploaded: {file_name}\")\n",
        "except (Exception, IndexError):\n",
        "    print(f\" File upload failed or was cancelled.\")\n",
        "    file_name = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "6_Ic-oEJnLVW",
        "outputId": "f03419a9-59a7-484f-f1d9-8a429bce1db5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---- Upload the file -----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-daafdb90-744a-4227-a159-029a94170f32\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-daafdb90-744a-4227-a159-029a94170f32\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving HSC26-Bangla1st-Paper.pdf to HSC26-Bangla1st-Paper.pdf\n",
            "Successfully uploaded: HSC26-Bangla1st-Paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  TEXT EXTRACTION WITH OCR (with Flexible Markers and Progress Indicator)\n",
        "def extract_story_with_ocr(pdf_path: str) -> str:\n",
        "    if not pdf_path: return \"\"\n",
        "\n",
        "    print(\"\\n Starting OCR-based text extraction...\")\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_ocr_text = \"\"\n",
        "    for page_num in range(5, 49): # Read pages 6 through last\n",
        "        if page_num < len(doc):\n",
        "            page = doc.load_page(page_num)\n",
        "            pix = page.get_pixmap(dpi=300)\n",
        "            image = Image.open(io.BytesIO(pix.tobytes()))\n",
        "\n",
        "            # This line shows which page is currently being processed\n",
        "            print(f\"  > Processing Page {page_num + 1} with OCR...\")\n",
        "\n",
        "            try:\n",
        "                text = pytesseract.image_to_string(image, lang='ben')\n",
        "                full_ocr_text += text + \"\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\" OCR failed on page {page_num + 1}: {e}\")\n",
        "                continue\n",
        "    doc.close()\n",
        "\n",
        "    if not full_ocr_text.strip():\n",
        "        print(\"\\n FATAL ERROR: OCR produced no text.\")\n",
        "        return \"\"\n",
        "\n",
        "    # ---  flexible markers ---\n",
        "    start_marker = \"আজ আমার বয়স সাতাশ\"  # More robust than the full sentence\n",
        "    end_marker = \"জায়গা পাইয়াছি\"        # More robust than the full sentence\n",
        "\n",
        "    try:\n",
        "        start_index = full_ocr_text.find(start_marker)\n",
        "        end_index = full_ocr_text.rfind(end_marker)\n",
        "\n",
        "        if start_index == -1 or end_index == -1:\n",
        "            print(\" FATAL ERROR: Story markers not found in OCR text.\")\n",
        "            return full_ocr_text\n",
        "\n",
        "        # End of the line for the end_marker to get the full sentence\n",
        "        end_marker_full_line_end = full_ocr_text.find('\\n', end_index)\n",
        "\n",
        "        story_text = full_ocr_text[start_index : end_marker_full_line_end]\n",
        "        story_text = re.sub(r'\\s*\\n\\s*', '\\n', story_text).strip()\n",
        "        print(\" OCR extraction and story isolation complete.\")\n",
        "        return story_text\n",
        "    except Exception as e:\n",
        "        print(f\" Error during text slicing: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# final extraction function\n",
        "story_text = extract_story_with_ocr(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9A1a_g6nQws",
        "outputId": "c5add019-92a3-4108-870d-e23f372ced92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Starting OCR-based text extraction...\n",
            "  > Processing Page 6 with OCR...\n",
            "  > Processing Page 7 with OCR...\n",
            "  > Processing Page 8 with OCR...\n",
            "  > Processing Page 9 with OCR...\n",
            "  > Processing Page 10 with OCR...\n",
            "  > Processing Page 11 with OCR...\n",
            "  > Processing Page 12 with OCR...\n",
            "  > Processing Page 13 with OCR...\n",
            "  > Processing Page 14 with OCR...\n",
            "  > Processing Page 15 with OCR...\n",
            "  > Processing Page 16 with OCR...\n",
            "  > Processing Page 17 with OCR...\n",
            "  > Processing Page 18 with OCR...\n",
            "  > Processing Page 19 with OCR...\n",
            "  > Processing Page 20 with OCR...\n",
            "  > Processing Page 21 with OCR...\n",
            "  > Processing Page 22 with OCR...\n",
            "  > Processing Page 23 with OCR...\n",
            "  > Processing Page 24 with OCR...\n",
            "  > Processing Page 25 with OCR...\n",
            "  > Processing Page 26 with OCR...\n",
            "  > Processing Page 27 with OCR...\n",
            "  > Processing Page 28 with OCR...\n",
            "  > Processing Page 29 with OCR...\n",
            "  > Processing Page 30 with OCR...\n",
            "  > Processing Page 31 with OCR...\n",
            "  > Processing Page 32 with OCR...\n",
            "  > Processing Page 33 with OCR...\n",
            "  > Processing Page 34 with OCR...\n",
            "  > Processing Page 35 with OCR...\n",
            "  > Processing Page 36 with OCR...\n",
            "  > Processing Page 37 with OCR...\n",
            "  > Processing Page 38 with OCR...\n",
            "  > Processing Page 39 with OCR...\n",
            "  > Processing Page 40 with OCR...\n",
            "  > Processing Page 41 with OCR...\n",
            "  > Processing Page 42 with OCR...\n",
            "  > Processing Page 43 with OCR...\n",
            "  > Processing Page 44 with OCR...\n",
            "  > Processing Page 45 with OCR...\n",
            "  > Processing Page 46 with OCR...\n",
            "  > Processing Page 47 with OCR...\n",
            "  > Processing Page 48 with OCR...\n",
            "  > Processing Page 49 with OCR...\n",
            " OCR extraction and story isolation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Verification of OCR Extraction\n",
        "if story_text and len(story_text) > 200:  # Check if the text is not empty and has substantial content\n",
        "    print(\" OCR Extraction Verified. The text appears to be clean and is ready for chunking.\")\n",
        "    print(\"\\n--- Sample of Verified OCR Text (first 500 characters) ---\")\n",
        "    print(story_text[:1500])\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "else:\n",
        "    print(\" Verification Failed: The text extracted via OCR is empty or too short.\")\n",
        "    print(\"    Troubleshooting steps:\")\n",
        "    print(\"    1. Ensure the correct PDF was uploaded.\")\n",
        "    print(\"    2. Check the 'Raw OCR Output' in the previous cell for any errors.\")\n",
        "    print(\"    3. Consider a 'Factory reset runtime' from the Colab menu and run all cells again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftQ0TN8ZDRKw",
        "outputId": "aa9ce012-f4e4-4be5-e8aa-f1d4316f448f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " OCR Extraction Verified. The text appears to be clean and is ready for chunking.\n",
            "\n",
            "--- Sample of Verified OCR Text (first 500 characters) ---\n",
            "আজ আমার বয়স সাতাশ মাত্র। এ জীবনটা না দৈর্ঘ্যের হিসাবে বড়, না গুনের হিসাবে। তবু ইহার একটু বিশেষ\n",
            "মূল্য আছে।ইহা সেই ফুলের মতো যাহার বুকের উপরে ভ্রমর আসিয়া বসিয়াছিল, এবং সেই পদক্ষেপের ইতিহাস\n",
            "তাহার জীবনের মাঝখানেফলের মতো গুটি ধরিয়া উঠিয়াছে।\n",
            "সেই ইতিহাসটুকু আকারে ছোটো, তাহাকে ছোটো করিয়াই লিখিব। ছোটোকে যাহারা সামান্য বলিয়া ভুল করেন\n",
            "না তাহারা ইহার রস বুঝিবেন। কলেজে যতগুলো পরীক্ষা পাশ করিবার সব আমি ঢুকাইয়াছি। ছেলেবেলায়\n",
            "আমাকে শিমুল ফুল ও মাকাল ফলের সহিত\n",
            "তুলনা করিয়া, বিদ্রপ করিবার সুযোগ\n",
            "পাইয়াছিলেন। ইহাতে তখন বড় লজ্জা পাইতাম;\n",
            "কিন্তু বয়স হইয়া এ কথা ভাবিয়াছি, যদি\n",
            "জন্মান্তর থাকে তবে আমার মুখে সুরূপ এবং\n",
            "পণ্ডিতমশায়দের মুখে বিদ্রপ আবার যেন অমনি করিইয়াই প্রকাশ পায়। আমার পিতা এক কালে গরিব ছিলেন।\n",
            "ওকালতি করিয়া তিনি প্রচুর টাকা রোজগার করিয়াছেন, ভোগ করিবার সময় নিমেষমাত্র পান নাই। মৃত্যুতে\n",
            "তিনি যে হাফ ছাড়িলেন সেই তার প্রথম অবকাশ।\n",
            "আমার তখন বয়স অল্প। মার হাতেই আমি মানুষ। মা গরিবের ঘরের মেয়ে; তাই, আমরা যে ধনী এ কথা তিনিও\n",
            "ভোলেন না, আমাকে ভুলিতে দেন না। শিশুকালে আমি কোলে কোলেই মানুষ-বোধ করি, সেইজন্য শেষ পর্যন্ত\n",
            "আমার পুরাপুরি বয়সই হইল না।\n",
            "আজও আমাকে দেখিলে মনে হইবে, আমি অন্নপূর্ণার কোলে গজাননের ছোটো ভাইটি।\n",
            "আমার আসল অভিভাবক আমার মামা। তিনি আমার চেয়ে বড়োজোর বছর ছয়েক বড়। কিন্তু ফল্তুর বালির\n",
            "মতো তিনি আমাদের সমস্ত সংসারটাকে নিজের অন্তরের মধ্যে শুষিয়া লইয়াছেন। তাহাকে না খুঁড়িয়া\n",
            "এখানকার এক গণ্ডষও রস পাইবার জো নাই। এই কারণে কোনো-কিছুর জন্যই আমাকে কোনো ভাবনা\n",
            "ভাবিতেই হয় না।কন্যার পিতা মাত্রেই স্বীকার করিবেন, আমি সৎপাত্র। তামাকটুকু পর্যন্ত খাই না। ভালোমানুষ\n",
            "হওয়ার কোনো ঝঞ্জাট নাই, তাই\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Chunking  the Story Text\n",
        "if story_text:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=750, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \"।\"]\n",
        "    )\n",
        "    chunks = text_splitter.split_text(story_text)\n",
        "    print(f\"\\n Story successfully split into {len(chunks)} high-quality chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDNojuN1nn0a",
        "outputId": "904c982d-b0cb-4636-8fdb-3729c0dc84b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Story successfully split into 41 high-quality chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    #  Embedding  Chunks with the Model Chroma VectorDB is used in here and LLM is Gemini\n",
        "    print(\"\\n Initializing RAG components with upgraded model...\")\n",
        "    embed_model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
        "    prefixed_chunks = [f\"passage: {chunk}\" for chunk in chunks]\n",
        "    chroma_client = chromadb.Client()\n",
        "    collection_name = \"oporichita_e5_final_pass\"\n",
        "    if len(chroma_client.list_collections()) > 0 and collection_name in [c.name for c in chroma_client.list_collections()]:\n",
        "        chroma_client.delete_collection(name=collection_name)\n",
        "    collection = chroma_client.create_collection(name=collection_name)\n",
        "    collection.add(\n",
        "        embeddings=embed_model.encode(prefixed_chunks).tolist(),\n",
        "        documents=chunks,\n",
        "        ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "    )\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "    print(\" RAG pipeline fully initialized.\")\n"
      ],
      "metadata": {
        "id": "M71MFlmdnrLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Generate Answer Function\n",
        "def generate_answer(query: str) -> str:\n",
        "    prefixed_query = f\"query: {query}\"\n",
        "    results = collection.query(\n",
        "        query_embeddings=[embed_model.encode(prefixed_query).tolist()],\n",
        "        n_results=5\n",
        "    )\n",
        "    context = \"\\n\\n---\\n\\n\".join(results['documents'][0])\n",
        "    prompt = f\"Based ONLY on the context below, answer the question with a single word or name. If not found, say 'উত্তর পাওয়া যায়নি'.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "v6nAp-47n8a9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Test Cases given\n",
        "if story_text:\n",
        "    print(\"\\n\\n--- Running Final Assessment Test Cases ---\")\n",
        "    test_cases = [\n",
        "        {\"q\": \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\", \"e\": \"শস্তুনাথবাবু\"},\n",
        "        {\"q\": \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\", \"e\": \"মামা\"},\n",
        "        {\"q\": \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\", \"e\": \"পনেরো\"},\n",
        "\n",
        "    ]\n",
        "    for case in test_cases:\n",
        "        actual_answer = generate_answer(case[\"q\"])\n",
        "        print(f\"\\n Question: {case['q']}\")\n",
        "        print(f\" Expected: {case['e']}\")\n",
        "        print(f\" RAG Answer: {actual_answer}\")\n",
        "        if case['e'] in actual_answer or actual_answer in case['e']:\n",
        "            print(\" Correct\")\n",
        "        else:\n",
        "            print(\"Incorrect\")\n",
        "        print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\nCannot run test cases because story extraction failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "Xz9xXbbmoFAP",
        "outputId": "cb19f1d7-a1d4-4929-c649-b129056da71e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Running Final Assessment Test Cases ---\n",
            "\n",
            " Question: অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\n",
            " Expected: শস্তুনাথবাবু\n",
            " RAG Answer: শস্তুনাথবাবু\n",
            " Correct\n",
            "==================================================\n",
            "\n",
            " Question: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            " Expected: মামা\n",
            " RAG Answer: মামা\n",
            " Correct\n",
            "==================================================\n",
            "\n",
            " Question: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
            " Expected: পনেরো\n",
            " RAG Answer: পনেরো\n",
            " Correct\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Running a New Random Test Case ---\n",
        "if collection:\n",
        "    print(\"\\n--- Testing with a new, random question ---\")\n",
        "\n",
        "    # Define the new question and expected answer\n",
        "    question = \"হরিশ কোথায় কাজ করে?\"\n",
        "    expected_answer = \"কানপুরে\"\n",
        "\n",
        "    # Generate the answer using your RAG pipeline\n",
        "    actual_answer = generate_answer(question)\n",
        "\n",
        "    # Print the results for verification\n",
        "    print(f\"\\n Question: {question}\")\n",
        "    print(f\" Expected: {expected_answer}\")\n",
        "    print(f\" RAG Answer: {actual_answer}\")\n",
        "\n",
        "    if expected_answer in actual_answer or actual_answer in expected_answer:\n",
        "        print(\" Correct\")\n",
        "    else:\n",
        "        print(\" Incorrect\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\nCannot run test case because the RAG pipeline is not ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "KzFSN966AixG",
        "outputId": "077723d3-d190-4d44-d8ba-9018e6bd4028"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing with a new, random question ---\n",
            "\n",
            " Question: হরিশ কোথায় কাজ করে?\n",
            " Expected: কানপুরে\n",
            " RAG Answer: কানপুর\n",
            " Correct\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: MODIFIED Generate Answer Function (Now with Memory)\n",
        "def generate_answer_with_memory(query: str, history: list) -> str:\n",
        "    \"\"\"\n",
        "    Generates an answer by considering both long-term (document) and short-term (chat) memory.\n",
        "    \"\"\"\n",
        "    # 1. Retrieve context from long-term memory (Vector DB)\n",
        "    prefixed_query = f\"query: {query}\"\n",
        "    results = collection.query(\n",
        "        query_embeddings=[embed_model.encode(prefixed_query).tolist()],\n",
        "        n_results=4\n",
        "    )\n",
        "    context = \"\\\\n\\\\n---\\\\n\\\\n\".join(results['documents'][0])\n",
        "\n",
        "    # 2. Format the short-term memory (chat history)\n",
        "    formatted_history = \"\\\\n\".join([f\"Human: {q}\\\\nAI: {a}\" for q, a in history])\n",
        "\n",
        "    # 3. Create the prompt with both memories\n",
        "    prompt = f\"\"\"You are a helpful assistant for the story 'Oporichita'.\n",
        "Answer the user's 'Human' question based on the 'Chat History' and the 'Retrieved Context'.\n",
        "Be concise and answer in Bengali.\n",
        "\n",
        "Chat History:\n",
        "{formatted_history}\n",
        "\n",
        "Retrieved Context:\n",
        "{context}\n",
        "\n",
        "Human: {query}\n",
        "AI:\"\"\"\n",
        "\n",
        "    # 4. Generate the response\n",
        "    response = model.generate_content(prompt)\n",
        "    clean_response = response.text.strip()\n",
        "\n",
        "    # 5. Update the history with the new interaction\n",
        "    history.append((query, clean_response))\n",
        "\n",
        "    return clean_response"
      ],
      "metadata": {
        "id": "jL2kvgBtUjYF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_chat():\n",
        "    \"\"\"\n",
        "    Initializes a chat session that uses the generate_answer_with_memory function.\n",
        "    \"\"\"\n",
        "    # Each new chat session starts with a fresh memory\n",
        "    chat_history = []\n",
        "    print(\"--- Chat with the AnupomaAI ---\")\n",
        "    print(\"Type 'exit' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"You: \")\n",
        "        # FIX: Add .strip() to remove whitespace before checking the input\n",
        "        if user_query.strip().lower() == 'exit':\n",
        "            print(\"AnupomaAI: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Pass the current session's history to the function\n",
        "        answer = generate_answer_with_memory(user_query, chat_history)\n",
        "        print(f\"AnupomaAI: {answer}\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "nATyU-C4UqaW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 11: NEW - Start the Chat\n",
        "# This cell will begin the interactive chat session.\n",
        "start_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "msfpeF_ezIRA",
        "outputId": "3e267991-c599-4a92-eebd-8b09dab64b10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chat with the AnupomaAI ---\n",
            "Type 'exit' to end the conversation.\n",
            "You: tell me a theme about the story in 2 sentence\n",
            "AnupomaAI: এই গল্পের মূল বিষয় হলো একজন পুরুষের এক রহস্যময়ী অচেনা নারীর প্রতি গভীর আকর্ষণ ও মুগ্ধতা। এই আকর্ষণ শুধু নারীর বাহ্যিক রূপেই সীমাবদ্ধ নয়, বরং তার সজীবতা, প্রাণবন্ততা এবং প্রকৃতির সাথে একাত্মতার অনুভূতিও তাকে বিশেষভাবে আকর্ষণ করে।\n",
            "--------------------------------------------------\n",
            "You: হরিশ কোথায় কাজ করে?\n",
            "AnupomaAI: হরিশ কানপুরে কাজ করে।\n",
            "--------------------------------------------------\n",
            "You:   Question: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে in 1 word \n",
            "AnupomaAI: মামা\n",
            "--------------------------------------------------\n",
            "You: exit \n",
            "AnupomaAI: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}